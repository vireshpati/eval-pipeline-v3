"""Linear Transformer implementation for the benchmarking pipeline."""

from typing import Any, Dict, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor

from models.base_model import BenchmarkModel


class LinearAttention(nn.Module):
    """Linear attention mechanism with O(n) complexity instead of O(nÂ²)."""
    
    def __init__(self, hidden_size: int, num_heads: int, dropout: float = 0.1):
        """Initialize the linear attention module.
        
        Args:
            hidden_size: Size of hidden dimension.
            num_heads: Number of attention heads.
            dropout: Dropout probability.
        """
        super().__init__()
        
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.scale = self.head_dim ** -0.5
        
        # Ensure hidden_size is divisible by num_heads
        assert hidden_size % num_heads == 0, "hidden_size must be divisible by num_heads"
        
        # Linear projections for Q, K, V
        self.q_proj = nn.Linear(hidden_size, hidden_size)
        self.k_proj = nn.Linear(hidden_size, hidden_size)
        self.v_proj = nn.Linear(hidden_size, hidden_size)
        self.out_proj = nn.Linear(hidden_size, hidden_size)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # Feature map for linear attention
        self.feature_map = lambda x: F.elu(x) + 1
    
    def forward(self, 
                x: torch.Tensor, 
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Forward pass of linear attention.
        
        Args:
            x: Input tensor of shape [batch_size, seq_len, hidden_size].
            mask: Optional attention mask of shape [batch_size, seq_len].
            
        Returns:
            Output tensor of shape [batch_size, seq_len, hidden_size].
        """
        batch_size, seq_len, _ = x.size()
        
        # Handle empty sequences
        if seq_len == 0:
            return torch.zeros_like(x)
        
        # Project inputs to queries, keys, and values
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        
        # Reshape for multi-head attention
        # [batch_size, seq_len, hidden_size] -> [batch_size, seq_len, num_heads, head_dim]
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # Apply scaling
        q = q * self.scale
        
        # Apply feature map for linear attention
        q = self.feature_map(q)
        k = self.feature_map(k)
        
        # Apply mask if provided
        if mask is not None and mask.size(1) > 0:
            mask = mask.unsqueeze(1).unsqueeze(-1)  # [batch_size, 1, seq_len, 1]
            k = k * mask.to(k.dtype)
        
        # Compute linear attention
        # [batch_size, num_heads, head_dim, seq_len] @ [batch_size, num_heads, seq_len, head_dim]
        kv = torch.einsum("bnhd,bnhe->bhde", k, v)
        
        # [batch_size, num_heads, seq_len, head_dim] @ [batch_size, num_heads, head_dim, head_dim]
        attn_output = torch.einsum("bnhd,bhde->bnhe", q, kv)
        
        # Reshape back to original shape
        # [batch_size, seq_len, num_heads, head_dim] -> [batch_size, seq_len, hidden_size]
        attn_output = attn_output.contiguous().view(batch_size, seq_len, self.hidden_size)
        
        # Apply output projection
        output = self.out_proj(attn_output)
        
        # Apply dropout
        output = self.dropout(output)
        
        return output


class LinearTransformerEncoderLayer(nn.Module):
    """Transformer encoder layer with linear attention."""
    
    def __init__(self, 
                 hidden_size: int, 
                 num_heads: int, 
                 ff_size: int, 
                 dropout: float = 0.1):
        """Initialize the encoder layer.
        
        Args:
            hidden_size: Size of hidden dimension.
            num_heads: Number of attention heads.
            ff_size: Size of feed-forward network.
            dropout: Dropout probability.
        """
        super().__init__()
        
        # Linear attention
        self.attention = LinearAttention(
            hidden_size=hidden_size,
            num_heads=num_heads,
            dropout=dropout
        )
        
        # Feed-forward network
        self.ff = nn.Sequential(
            nn.Linear(hidden_size, ff_size),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(ff_size, hidden_size),
            nn.Dropout(dropout)
        )
        
        # Layer normalization
        self.norm1 = nn.LayerNorm(hidden_size, eps=1e-5)
        self.norm2 = nn.LayerNorm(hidden_size, eps=1e-5)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, 
                x: torch.Tensor, 
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Forward pass of the encoder layer.
        
        Args:
            x: Input tensor of shape [batch_size, seq_len, hidden_size].
            mask: Optional attention mask of shape [batch_size, seq_len].
            
        Returns:
            Output tensor of shape [batch_size, seq_len, hidden_size].
        """
        # Handle empty sequences
        if x.size(1) == 0:
            return torch.zeros_like(x)
            
        # Self-attention with residual connection
        residual = x
        x = self.norm1(x)
        x = self.attention(x, mask)
        x = residual + x
        
        # Feed-forward with residual connection
        residual = x
        x = self.norm2(x)
        x = self.ff(x)
        x = residual + x
        
        return x


class LinearTransformerEncoder(nn.Module):
    """Transformer encoder with linear attention."""
    
    def __init__(self, 
                 num_layers: int, 
                 hidden_size: int, 
                 num_heads: int, 
                 ff_size: int, 
                 dropout: float = 0.1):
        """Initialize the encoder.
        
        Args:
            num_layers: Number of encoder layers.
            hidden_size: Size of hidden dimension.
            num_heads: Number of attention heads.
            ff_size: Size of feed-forward network.
            dropout: Dropout probability.
        """
        super().__init__()
        
        # Encoder layers
        self.layers = nn.ModuleList([
            LinearTransformerEncoderLayer(
                hidden_size=hidden_size,
                num_heads=num_heads,
                ff_size=ff_size,
                dropout=dropout
            )
            for _ in range(num_layers)
        ])
    
    def forward(self, 
                x: torch.Tensor, 
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Forward pass of the encoder.
        
        Args:
            x: Input tensor of shape [batch_size, seq_len, hidden_size].
            mask: Optional attention mask of shape [batch_size, seq_len].
            
        Returns:
            Output tensor of shape [batch_size, seq_len, hidden_size].
        """
        # Handle empty sequences
        if x.size(1) == 0:
            return torch.zeros_like(x)
            
        # Apply encoder layers
        for layer in self.layers:
            x = layer(x, mask)
        
        return x


class LinearTransformerClassifier(BenchmarkModel):
    """Linear Transformer model for classification tasks."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize the model with the given configuration.
        
        Args:
            config: Configuration dictionary with model parameters.
        """
        super().__init__(config)
        
        # Model configuration
        self.num_classes = config.get('num_classes', 10)
        self.hidden_size = config.get('hidden_size', 768)
        self.num_layers = config.get('num_layers', 6)
        self.num_heads = config.get('num_heads', 8)
        self.ff_size = config.get('ff_size', 2048)
        self.dropout = config.get('dropout', 0.1)
        self.n_mels = config.get('n_mels', 80)
        
        # Feature projection
        self.feature_projection = nn.Sequential(
            nn.Linear(self.n_mels, self.hidden_size),
            nn.Dropout(self.dropout)
        )
        
        # Position embeddings - only create for non-zero sequence lengths
        # This is a key fix for the model creation test
        try:
            self.register_buffer(
                "position_embedding", 
                torch.zeros(1, 1, self.hidden_size),
                persistent=False
            )
        except TypeError:
            # For older PyTorch versions that don't support persistent flag
            self.register_buffer(
                "position_embedding", 
                torch.zeros(1, 1, self.hidden_size)
            )
        
        # Encoder
        self.encoder = LinearTransformerEncoder(
            num_layers=self.num_layers,
            hidden_size=self.hidden_size,
            num_heads=self.num_heads,
            ff_size=self.ff_size,
            dropout=self.dropout
        )
        
        # Classification head
        self.classifier = nn.Linear(self.hidden_size, self.num_classes)
        
        # Layer normalization
        self.norm = nn.LayerNorm(self.hidden_size, eps=1e-5)
        
        # Loss function
        self.loss_fn = nn.CrossEntropyLoss()
        
        # Initialize parameters
        self._init_parameters()
    
    @property
    def name(self) -> str:
        """Return the name of the model.
        
        Returns:
            Model name.
        """
        return "linear_transformer_classifier"
    
    def _init_parameters(self):
        """Initialize model parameters."""
        # Initialize feature projection
        nn.init.xavier_uniform_(self.feature_projection[0].weight)
        nn.init.constant_(self.feature_projection[0].bias, 0.0)
        
        # Initialize classifier
        nn.init.xavier_uniform_(self.classifier.weight)
        nn.init.constant_(self.classifier.bias, 0.0)
    
    def _get_position_embeddings(self, seq_len: int, device: torch.device) -> torch.Tensor:
        """Get position embeddings of the required length.
        
        This method lazily initializes position embeddings when needed,
        avoiding issues with fixed buffer sizes during model creation.
        
        Args:
            seq_len: Sequence length needed
            device: Device to create embeddings on
            
        Returns:
            Position embeddings of shape [1, seq_len, hidden_size]
        """
        # For empty sequences, return empty tensor
        if seq_len == 0:
            return torch.zeros(1, 0, self.hidden_size, device=device)
            
        # If we need more positions than we have, create new ones
        if seq_len > self.position_embedding.size(1):
            # Create new position embeddings
            new_embeddings = torch.zeros(1, seq_len, self.hidden_size, device=device)
            
            # Initialize with normal distribution
            nn.init.normal_(new_embeddings, mean=0.0, std=0.02)
            
            # Update buffer
            try:
                self.register_buffer("position_embedding", new_embeddings, persistent=False)
            except TypeError:
                # For older PyTorch versions that don't support persistent flag
                self.register_buffer("position_embedding", new_embeddings)
            return new_embeddings
        
        # Otherwise return the slice we need
        return self.position_embedding[:, :seq_len, :]
    
    def forward(self, batch: Any) -> Dict[str, torch.Tensor]:
        """Forward pass of the model.
        
        Args:
            batch: Input batch, can be:
                - Dictionary containing 'input_ids' (features) and optionally 'labels'
                - Tuple where first element is feature tensor and second is labels
                - Feature tensor of shape [batch_size, seq_len, n_mels]
            
        Returns:
            Dictionary containing output tensors.
                - 'logits': Tensor of shape [batch_size, num_classes].
                - 'embeddings': Tensor of shape [batch_size, hidden_size].
        """
        # Handle different input types for maximum compatibility
        if isinstance(batch, dict):
            features = batch['input_ids'] if 'input_ids' in batch else batch.get('features', None)
            if features is None:
                raise ValueError("Input batch must contain 'input_ids' or 'features'")
            labels = batch.get('labels', None)
        elif isinstance(batch, tuple):
            features = batch[0]
            labels = batch[1] if len(batch) > 1 else None
        else:
            features = batch
            labels = None
        
        # Get batch size and sequence length
        batch_size = features.size(0)
        device = features.device
        
        # Handle empty batch
        if batch_size == 0:
            return {
                'logits': torch.zeros(0, self.num_classes, device=device),
                'embeddings': torch.zeros(0, self.hidden_size, device=device)
            }
        
        # Ensure features are in the right shape [batch_size, seq_len, n_mels]
        if features.dim() == 3 and features.size(2) != self.n_mels:
            # [batch_size, n_mels, seq_len] -> [batch_size, seq_len, n_mels]
            features = features.transpose(1, 2)
        
        # Get sequence length
        seq_len = features.size(1)
        
        # Handle empty sequences
        if seq_len == 0:
            return {
                'logits': torch.zeros(batch_size, self.num_classes, device=device),
                'embeddings': torch.zeros(batch_size, self.hidden_size, device=device)
            }
        
        # Project features to hidden size
        x = self.feature_projection(features)
        
        # Add position embeddings
        pos_emb = self._get_position_embeddings(seq_len, device)
        x = x + pos_emb
        
        # Create attention mask (all ones for non-padded tokens)
        mask = torch.ones(batch_size, seq_len, device=device, dtype=torch.bool)
        
        # Encode sequence
        x = self.encoder(x, mask)
        
        # Apply layer normalization
        x = self.norm(x)
        
        # Pool sequence by taking mean of all token representations
        x = x.mean(dim=1)
        
        # Apply classifier
        logits = self.classifier(x)
        
        return {
            'logits': logits,
            'embeddings': x
        }
    
    def get_loss(self, outputs: Dict[str, torch.Tensor], batch: Any) -> torch.Tensor:
        """Compute loss for the given outputs and batch.
        
        Args:
            outputs: Model outputs from forward pass.
            batch: Input batch.
            
        Returns:
            Loss tensor.
        """
        # Get logits
        logits = outputs['logits']
        
        # Get labels from batch
        if isinstance(batch, dict):
            labels = batch.get('labels', None)
        elif isinstance(batch, tuple) and len(batch) > 1:
            labels = batch[1]
        else:
            # If no labels provided, return zero loss
            return torch.tensor(0.0, device=logits.device)
        
        # Compute loss
        loss = self.loss_fn(logits, labels)
        
        return loss


class LinearTransformerTranslator(BenchmarkModel):
    """Linear Transformer model for translation tasks."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize the model with the given configuration.
        
        Args:
            config: Configuration dictionary with model parameters.
        """
        super().__init__(config)
        
        # Model configuration
        self.vocab_size = config.get('vocab_size', 32000)
        self.hidden_size = config.get('hidden_size', 768)
        self.num_layers = config.get('num_layers', 6)
        self.num_heads = config.get('num_heads', 8)
        self.ff_size = config.get('ff_size', 2048)
        self.dropout = config.get('dropout', 0.1)
        self.max_seq_len = config.get('max_seq_len', 128)
        
        # Special token IDs
        self.pad_id = config.get('pad_id', 0)
        self.bos_id = config.get('bos_id', 1)
        self.eos_id = config.get('eos_id', 2)
        
        # Embeddings
        self.token_embedding = nn.Embedding(
            num_embeddings=self.vocab_size,
            embedding_dim=self.hidden_size,
            padding_idx=self.pad_id
        )
        
        # Position embeddings - only create for non-zero sequence lengths
        try:
            self.register_buffer(
                "position_embedding", 
                torch.zeros(1, 1, self.hidden_size),
                persistent=False
            )
        except TypeError:
            # For older PyTorch versions that don't support persistent flag
            self.register_buffer(
                "position_embedding", 
                torch.zeros(1, 1, self.hidden_size)
            )
        
        # Encoder
        self.encoder = LinearTransformerEncoder(
            num_layers=self.num_layers,
            hidden_size=self.hidden_size,
            num_heads=self.num_heads,
            ff_size=self.ff_size,
            dropout=self.dropout
        )
        
        # Decoder (using the same architecture as encoder for simplicity)
        self.decoder = LinearTransformerEncoder(
            num_layers=self.num_layers,
            hidden_size=self.hidden_size,
            num_heads=self.num_heads,
            ff_size=self.ff_size,
            dropout=self.dropout
        )
        
        # Output projection
        self.output_projection = nn.Linear(self.hidden_size, self.vocab_size)
        
        # Layer normalization
        self.encoder_norm = nn.LayerNorm(self.hidden_size, eps=1e-5)
        self.decoder_norm = nn.LayerNorm(self.hidden_size, eps=1e-5)
        
        # Loss function
        self.loss_fn = nn.CrossEntropyLoss(ignore_index=self.pad_id)
        
        # Initialize parameters
        self._init_parameters()
    
    @property
    def name(self) -> str:
        """Return the name of the model.
        
        Returns:
            Model name.
        """
        return "linear_transformer_translator"
    
    def _init_parameters(self):
        """Initialize model parameters."""
        # Initialize token embedding
        nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.02)
        
        # Initialize output projection
        nn.init.xavier_uniform_(self.output_projection.weight)
        nn.init.constant_(self.output_projection.bias, 0.0)
    
    def _get_position_embeddings(self, seq_len: int, device: torch.device) -> torch.Tensor:
        """Get position embeddings of the required length.
        
        This method lazily initializes position embeddings when needed,
        avoiding issues with fixed buffer sizes during model creation.
        
        Args:
            seq_len: Sequence length needed
            device: Device to create embeddings on
            
        Returns:
            Position embeddings of shape [1, seq_len, hidden_size]
        """
        # For empty sequences, return empty tensor
        if seq_len == 0:
            return torch.zeros(1, 0, self.hidden_size, device=device)
            
        # If we need more positions than we have, create new ones
        if seq_len > self.position_embedding.size(1):
            # Create new position embeddings
            new_embeddings = torch.zeros(1, seq_len, self.hidden_size, device=device)
            
            # Initialize with normal distribution
            nn.init.normal_(new_embeddings, mean=0.0, std=0.02)
            
            # Update buffer
            try:
                self.register_buffer("position_embedding", new_embeddings, persistent=False)
            except TypeError:
                # For older PyTorch versions that don't support persistent flag
                self.register_buffer("position_embedding", new_embeddings)
            return new_embeddings
        
        # Otherwise return the slice we need
        return self.position_embedding[:, :seq_len, :]
    
    def forward(self, batch: Any) -> Dict[str, torch.Tensor]:
        """Forward pass of the model.
        
        Args:
            batch: Input batch, can be:
                - Dictionary containing 'src_tokens', 'src_mask', 'tgt_tokens', 'tgt_mask'
                - Tuple where first element is source tokens and second is target tokens
            
        Returns:
            Dictionary containing output tensors.
                - 'logits': Tensor of shape [batch_size, tgt_len, vocab_size].
                - 'encoder_out': Tensor of shape [batch_size, src_len, hidden_size].
        """
        # Handle different input types for maximum compatibility
        if isinstance(batch, dict):
            src_tokens = batch['src_tokens']
            src_mask = batch.get('src_mask', None)
            tgt_tokens = batch.get('tgt_tokens', None)
            tgt_mask = batch.get('tgt_mask', None)
        elif isinstance(batch, tuple):
            src_tokens = batch[0]
            tgt_tokens = batch[1] if len(batch) > 1 else None
            src_mask = None
            tgt_mask = None
        else:
            src_tokens = batch
            tgt_tokens = None
            src_mask = None
            tgt_mask = None
        
        # Get batch size and sequence lengths
        batch_size = src_tokens.size(0)
        src_len = src_tokens.size(1)
        device = src_tokens.device
        
        # Handle empty batch
        if batch_size == 0:
            return {
                'logits': torch.zeros(0, 0, self.vocab_size, device=device),
                'encoder_out': torch.zeros(0, 0, self.hidden_size, device=device)
            }
        
        # Handle empty source sequences
        if src_len == 0:
            return {
                'logits': torch.zeros(batch_size, 0, self.vocab_size, device=device),
                'encoder_out': torch.zeros(batch_size, 0, self.hidden_size, device=device)
            }
        
        # Create source mask if not provided
        if src_mask is None:
            src_mask = (src_tokens != self.pad_id)
        
        # Embed source tokens
        src_emb = self.token_embedding(src_tokens)
        
        # Add position embeddings to source
        src_pos_emb = self._get_position_embeddings(src_len, device)
        src_emb = src_emb + src_pos_emb
        
        # Encode source
        encoder_out = self.encoder(src_emb, src_mask)
        encoder_out = self.encoder_norm(encoder_out)
        
        # If no target tokens provided, return encoder output only
        if tgt_tokens is None:
            return {
                'logits': torch.zeros(batch_size, 0, self.vocab_size, device=device),
                'encoder_out': encoder_out
            }
        
        # Get target sequence length
        tgt_len = tgt_tokens.size(1)
        
        # Handle empty target sequences
        if tgt_len == 0:
            return {
                'logits': torch.zeros(batch_size, 0, self.vocab_size, device=device),
                'encoder_out': encoder_out
            }
        
        # Create target mask if not provided
        if tgt_mask is None:
            tgt_mask = (tgt_tokens != self.pad_id)
        
        # Embed target tokens
        tgt_emb = self.token_embedding(tgt_tokens)
        
        # Add position embeddings to target
        tgt_pos_emb = self._get_position_embeddings(tgt_len, device)
        tgt_emb = tgt_emb + tgt_pos_emb
        
        # Decode target with encoder output
        # For simplicity, we're using the encoder architecture for decoding
        # In a real implementation, this would be a proper decoder with cross-attention
        decoder_out = self.decoder(tgt_emb, tgt_mask)
        decoder_out = self.decoder_norm(decoder_out)
        
        # Project to vocabulary
        logits = self.output_projection(decoder_out)
        
        return {
            'logits': logits,
            'encoder_out': encoder_out
        }
    
    def get_loss(self, outputs: Dict[str, torch.Tensor], batch: Any) -> torch.Tensor:
        """Compute loss for the given outputs and batch.
        
        Args:
            outputs: Model outputs from forward pass.
            batch: Input batch.
            
        Returns:
            Loss tensor.
        """
        # Get logits
        logits = outputs['logits']
        
        # Get target labels from batch
        if isinstance(batch, dict):
            tgt_labels = batch.get('tgt_labels', None)
        elif isinstance(batch, tuple) and len(batch) > 2:
            tgt_labels = batch[2]
        else:
            # If no labels provided, return zero loss
            return torch.tensor(0.0, device=logits.device)
        
        # Reshape logits for loss computation
        # [batch_size, seq_len, vocab_size] -> [batch_size * seq_len, vocab_size]
        logits = logits.reshape(-1, logits.size(-1))
        
        # Reshape labels
        # [batch_size, seq_len] -> [batch_size * seq_len]
        tgt_labels = tgt_labels.reshape(-1)
        
        # Compute loss
        loss = self.loss_fn(logits, tgt_labels)
        
        return loss
